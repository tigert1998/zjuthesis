\section{神经网络设计上的案例研究}
\label{case studies}

\subsection{通道剪枝}
通道剪枝中逐层的通道剪枝率是很难选择的。
启发式的和基于NAS的方法都根据每层中各通道的重要性来设置通道配置。
这个过程需要大量的时间以达到最佳的准确度和效率平衡。

节\ref{analysis:channels}的发现表明，$C_{out}$的减少并不总是意味着延时的减少。
因此我们可以通过只选择延时台阶的最大通道数，并跳过台阶中的其他通道数来保持准确度、加速通道剪枝。

例如，MetaPruning\cite{liu2019metapruning}
对MobileNetV1中的每一层$l$，
在$[\lfloor 0.1C_{out}^l \rfloor, C_{out}^l]$的范围内，
以$\lfloor 0.03C_{out}^l \rfloor$为步长搜索$C_{out}$
（其中$C_{out}^l$是原始网络中第$l$层的通道数）。
对于输出张量为$112^2\times 32$的一层，
初始的步长是1，候选的通道数是30（从3到32）。
那么包含14层的MobileNetV1的搜索空间大小将大约是$30^{14}$。
幸运的是，本文的发现展示了，
在CPU上卷积的推理延时随$C_{out}$的减少是台阶式的，并且台阶的长度是8
（如图\ref{fig:conv_cin_fixed:cpu}所示）。
这将候选的通道数选择量从30减少到4（即8、16、24、32），
将搜索空间从$30^{14}$减少到$4^{14}$。
在单层中我们将通道数选择量降低到了原来的$7.5^{-1}$。
并且我们通过将总共的搜索空间大小降低到了原来的$10^{-12}$，来加速MobileNetV1的剪枝过程。

\subsection{可感知硬件NAS的指南}
搜索对硬件友好模型的常见做法是在NAS的搜索过程中应用OPs或延迟约束
\cite{cai2018proxylessnas, wu2019fbnet}。
对硬件多样性的认识不足会误导这些工作对所有硬件使用相同的针对CPU手工构造的搜索空间。
节\ref{analysis:op block}和节\ref{analysis:model quantization}
中的发现揭示了这些手工设计的搜索空间中低效的部分。
并且，我们依此提出了一些针对NAS搜索空间的指南。

我们首先展示一个减少搜索空间大小的例子。
最近的可感知硬件NAS方法采用层级别的搜索空间，
其中每层从多种模块（例如，具有不同内核大小的MobileNet和ShuffleNet模块变体）中
选择最优的算子/模块。
上述的搜索空间通常庞大而且搜索代价昂贵。
例如，MnasNet的搜索空间大小是$10^{13}$，
其搜索成本是40000 GPU小时\cite{tan2019mnasnet}。
如节\ref{analysis:op block:block with hardware}所述，
SE模块显著增加了除CPU外的所有其他硬件上的推理延时。
因此，我们建议从MnasNet的AI加速器搜索空间中删除SE。
这可以将搜索空间大小减少到原先的$32^{-1}$。

对于未来的搜索空间设计，我们总结了以下指南：
\begin{enumerate*}
    \item 每个硬件都需要定制化的神经网络搜索空间；
    \item CPU更偏好计算量较少的模块，而AI加速器更偏好数据重用率较高的模块；
    \item 新模块的设计应当考虑到目标软硬件上算子融合和量化算法的兼容性。
\end{enumerate*}
