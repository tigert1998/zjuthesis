\section{基准测试套件和测量方法}
\label{methodology}
本文的目的是定量地分析设计空间中各个维度上的神经网络硬件的行为，
来为在边缘设备上构建高效网络提供全面的见解和改进方针。
合理的基准测试选择（节\ref{suite}）和严格的测量方法（节\ref{measurement}）是实现此目标的前提。

\subsection{基准测试套件}
\label{suite}

\paragraph{通道数}
如节\ref{nn design and deployment}所述，通道数对于边缘设备上的模型可用性至关重要。
除了启发式的规则之外，基于剪枝和搜索的方法都广泛使用OPs作为通道数的一个硬约束，
并假设较少的通道数（即较少的OPs）意味着更快的推理速度。
但是真实设备上的推理延时是否符合这样的假设呢？
为了回答这个问题，表todo所列的基准测试套件内包含通道数的类别。

此类别主要关注卷积（Conv）和逐通道卷积（DWConv）算子，因为它们占据了CNN推理的大部分延时。
它包含三种配置：
\begin{enumerate*}[label=(\alph*)]
    \item 固定输入通道数$C_{in}$，并变化输出通道数$C_{out}$；
    \item 固定输出通道数$C_{out}$，并变化输入通道数$C_{in}$；
    \item 在逐通道卷积上变化通道数（$C_{in}=C_{out}$）。
\end{enumerate*}
通道数的范围是从主流的高效CNN模型中总结出来的。
该类别也包括了特征图尺寸（从$224^2$到$7^2$）和内核尺寸（1，3，5，7）的维度。

\paragraph{算子和模块}
算子和模块是神经网络模型设计的核心。
我们的基准测试套件包含了丰富的网络结构池。

在算子方面，套件包括常用的逐元素算子，卷积以及一系列激活函数。
ReLU、ReLU6和Sigmoid是传统的激活函数。
近年来Swish\cite{ramachandran2017swish}也被用于代替ReLU以提高准确率。
HardSwish用近似的方法来降低Swish的计算复杂性。
在模块方面，套件包括了节\ref{nn design and deployment}讨论的来自SOTA模型的有代表性的模块（如图todo所示）。
套件还包含不同的特征图尺寸和内核尺寸以进行综合分析。

影响神经网络硬件行为的一个重要因素是
算子的数据重用率（Data Reuse Rate），即计算密度（Operational Intensity）。
表todo显示了典型算子具有的OPs和内存访问开销（Memory Access Cost）的表达式。
显然，卷积是最计算密集的算子，因而其性能往往受硬件计算带宽的限制。
对于分组卷积，因为其特征图被按组（$G$）划分计算以减少OPs，所以其数据重用率较低。
DWConv可以看作$G = C_{in}$的分组卷积。
它的数据重用率常是Conv的几十分之一。
逐元素算子和激活函数对张量的每个元素都执行例如加或乘的操作。
他们是最内存密集型的算子，其性能对AI加速器的内存带宽非常敏感。

影响硬件行为的另一个因素是模块结构。
例如，逐元素算子在网络中的位置决定了它们是否可以与Conv融合以减少内存访问。
我们将在节todo内详细解释这些因素是如何影响神经网络行为的。

\paragraph{模型与量化}
套件内还包含了14个CNN模型，包括了最具代表性的手工设计的模型和NAS搜索的模型。
\begin{itemize}
    \item 手工设计的模型：适用于移动端设备的MobileNetV1/V2、ShuffleNetV1/V2，
    以及大模型如ResNetV1/V2、InceptionV1\cite{szegedy2015going}。
    \item 搜索得到的模型：MobileNetV3-Large-1.0\cite{howard2019searching}、
    Proxyless-Mobile/Mobile-14\cite{cai2018proxylessnas}、
    MnasNet-A1\cite{tan2019mnasnet}、
    NasNet-A-Mobile\cite{zoph2018learning}、
    EfficientNet-B0/B1\cite{tan2019efficientnet}
\end{itemize}
这些模型具有不同的计算、访存复杂度。
它们的操作数从140 MOPs（ShuffleNetV1）到6.55 GOPs（ResNetV2）不等。
模型大小从9 MB（ShuffleNetV2）到99 MB（ResNetV2）不等。
基准测试包括了各硬件支持的所有精度，例如AI加速器上的FP16和INT8（表todo），
来评估量化算法对各模型加速比和准确度的影响。

\subsection{测量方法}
\label{measurement}
本文的工作在七种AI处理器测量了上述的基准测试套件来分析神经网络硬件的行为。
本节介绍了测试平台，设置和以及延时/准确度的评估方法。

\paragraph{处理器和框架}
表todo列出了本文评估的处理器和对应的部署框架。
表中列出的精度是处理器与其软件框架都支持的精度。
硬件峰值性能一栏是制造商宣称的每秒操作数量。
因为缺少官方数据，Adreno GPU的峰值性能是由性能测试工具\cite{clpeak}得到的。
另外，本文使用单核CPU测试以避免大小核调度。

除了TFLite，其他软件都是由硬件厂商开发的专有框架。
用于DSP的Hexagon NNLib是开源的，但SNPE的其他部分都是闭源的。
在诸多可以部署到移动端CPU、GPU的推理框架中，我们选择了TFLite因为其应用最为广泛\cite{xu2019first}。
设备一栏是我们对各处理器进行实验中使用的实际硬件产品。
最后一栏是下文中对各处理器以及其框架的缩写。

\paragraph{延时测量}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.8\linewidth]{final/tool}
    \caption{\label{fig:tool}延时测量流程}
\end{figure}

如图\ref{fig:tool}所示，我们开发了一个自动执行基准测试的工具。
对于算子或模块基准测试，该工具首先会生成一个单层算子或单模块的TensorFlow模型。
然后该工具将调用各框架API将TensorFlow图转换为目标框架格式，
例如SNPE的\emph{snpe-tensorflow-to-dlc}。
另外在INT8量化中，该工具将利用ImageNet中的图片作为校准数据。
经过模型转换后，生成的计算图将被推送到目标处理器并在对应框架运行时中进行推理。

算子/模块分析的目标是帮助神经网络模型设计。
因而测得的延时应当与该算子/模块在完整的神经网络模型中的延时一致。
对AI加速器而言，在执行一次推理前后往往有宿主机-加速器之间的数据传输开销。
此开销需要被排除在算子/模块测量中。
幸运的是，除了TFLite的加速器后端之外，每个框架的算子性能分析工具都分别提供了数据传输和算子计算的延时。
我们直接使用框架提供的该信息进行分析。
由于这些框架都顺序地计算各算子，我们直接计算模块的延时为其构成算子的延时之和。

当前TFLite没有为GPU、Edge TPU提供逐算子的分析工具。
因此在TFLite GPU后端上，我们通过OpenCL Profiling Event实现了算子性能分析工具来
分别记录数据传输和GPU计算延时。
由于TFLite的Edge TPU后端是闭源的并且仅提供模型的整体延时，我们采用一个折中的方法计算单层延时：
我们将相同的算子/模块堆叠为两个多层的模型，例如一个40层模型和另一个10层模型，
继而得出单层模型的延时为将两个模型的延时差除以它们的深度之差。

对于神经网络模型的整体延时测试，
我们在测量过程中关闭了算子性能分析工具并直接报告真实部署中模型端到端的延时。