\cleardoublepage

\section{引言}
许多边缘设备，如智能手机、相机、扬声器和传感器为神经网络技术提供了理想的实际使用场景，
例如语音助手，智能安防和自动驾驶。
与将数据发送到云端服务器相比，在端侧上的推理对于保护用户隐私，以及构建始终可用且响应迅速的AI至关重要。
因此，将神经网络部署在端侧上的需求和动力是巨大的。

近年来，端侧神经网络部署的整个技术栈都取得了巨大的进展，包括硬件、推理框架和高效的神经网络结构。
其中在AI硬件上，不少传统科技公司或创业公司凭借他们的产品占据了一席之地，
例如Google的Edge TPU\cite{edgetpu}，英特尔的Movidius VPU\cite{myriad}，Canaan的KPU\cite{k210}
和寒武纪的MLU\cite{cambricon}。
这是整个半导体领域第一次经历这种大规模的扩张和快速迭代。
边缘端AI开发框架的发展也是迅速的，例如开源且被广泛使用的TensorFlow Lite（TFLite）\cite{tflite}
和PyTorch Caffe2\cite{caffe2}，以及专用硬件供应商为其芯片产品提供的闭源推理框架等。
例如，高通开发的用于骁龙SoC的SNPE（骁龙神经处理引擎）运行时\cite{snpe}。
硬件和软件框架的多样性给神经网络算法设计带来了巨大的挑战。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.8\linewidth]{final/pipeline}
    \caption{\label{fig:pipeline}标准的端侧神经网络算法部署流程}
\end{figure}

本质上，神经网络设计的重点是提高其准确率。
如图\ref{fig:pipeline}所示，神经网络的设计空间涉及了许多维度。
为了在边缘设备上高效推理神经网络，设计空间中也要加入为效率而考虑的约束。
但是，人们对目前约束有一定的误解。
就比如最常见的约束：计算操作数
（OPs\footnote{因为不同AI芯片会采用不同的推理精度，所以本文使用OPs而不是FLOPs。
这里OPs的定义和\cite{zhang2018shufflenet}一致，即乘加的数量。}）。
许多工作只是旨在通过减少OPs来降低延时。
不幸的是，较少的OPs并不直接意味着延时的减少，相反可能会损害模型的准确性。
例如，我们的测量表明，在Edge TPU上，MobileNetV3（209MOPs）比MobileNetEdgeTPU（990 MOPs）慢，并且模型准确率更低。
另一个误解是，一个高效的模型可以在每种设备都做到快速推理。
同样，事实并非如此。
根据我们的数据，MobileNetV3在ARM Cortex A76上的运行速度比MobileNetV2快25\%，
但前者在Movidius VPU比后者慢71\%。

一些神经网络设计的工作\cite{dai2019chamnet, tan2019mnasnet, yang2018netadapt}注意到了这些误解，
并在设计过程中测量了目标硬件上的实际延迟作为优化目标。
但是，考虑到大量的超参数、算子和组成模块种类，利用穷举式的测量方法将每个模型部署在目标硬件上是昂贵的。
因此，如何展示出神经网络硬件的行为特征并得出对神经网络设计空间的有效约束是一个需要解决的重要问题。

高效的算法设计通常依赖于专业硬件知识和性能分析工具如性能计数器、性能预测模型。
但是，由于AI处理器是多样化的而且它们中的许多连同其耦合的开发框架是闭源的，以上方法都不可行。

为了在缺乏硬件知识的情况下帮助高效神经网络的设计，我们构建了一个基准测试套件：\sysname。
它考虑了神经网络设计中主要维度，如流行的基本构成模块、通道数、特征图尺寸、内核尺寸、激活函数、量化方法
以及整体的神经网络模型。
测量结果定量地反映了特定AI芯片在神经网络负载上表现出来的特性，并展示对神经网络设计的启示。
例如，HardSwish\cite{howard2019searching}激活函数虽然旨在减少计算量，
但在Rockchip NPU\cite{rk3399pro}上运行延时比其他激活函数至多高13倍。
因此，我们可以直接在该芯片的神经网络设计空间中删除HardSwish。
本文将着重分析卷积神经网络（CNN）由于其复杂的设计空间。

前人还提出了其他基准测试，例如
BenchIP\cite{tao2017benchip}和DeepBench\cite{deepbench}。
但是，这些工作的目标与本文有差异。
他们的目标是通过运行计算密集型算子或整个模型来比较AI芯片的性能，但并没有回答如何为特定硬件设计神经网络的问题。
例如哪些通道数的选择是高效的？每个神经网络层最快的构建模块是什么？
较大的内核尺寸是否意味着更长的推理时间？

我们针对七种代表性的AI处理器，包括Edge TPU，VPU，NPU，KPU等（todo）来分析不同的神经网络行为。
令人惊讶的是，对比这些算子/模块本身的计算访存复杂度，在测试中它们中的许多有着反直觉的延时表现，
这也进一步体现了神经网络基准测试的必要性。
本文中，我们展示了八个发现以及一系列实践指南。
我们深入地通过硬件特性、框架实现和神经网络结构解释了这些发现所产生的原因。
为了展示如何应用上述的发现，本文还包括了实际的神经网络设计案例。
本文的工作是我们已知的，第一次在不同设计空间维度上，综合的面向一系列真实AI芯片的，针对神经网络硬件行为的分析。

从这些发现中总结出的主要结论是
\begin{enumerate*}
    \item 卷积通道数和内核尺寸的增加不总是会导致延时的增加。
    比如，除KPU之外，卷积的延时随着输出通道数台阶式地而不是线性式地上升。
    \item 神经网络算子和组成模块的延时在不同硬件上的特性截然不同。
    比如，在MobileNetV2添加SE\cite{hu2018squeeze}组成模块几乎不会增加CPU推理延时，
    但是会增加16倍的KPU推理延时以及52倍的TPU推理延时。
    \item 一些量化算法会减慢特定算子的推理速度以及会影响模型精度。
    加法算子在INT8量化下比FP32推理慢多达5倍。
    \item 一些设计良好的轻量化模型仅仅在CPU上表现良好。
    在AI加速器上（比如Rockchip NPU），MobileNetV3的推理速度甚至比单核ARM CPU还要慢。
    另外其模型的精度也大幅度下降。
\end{enumerate*}

通过神经网络的行为分析，本文也展示了硬件的改进空间。
很重要的一点是，许多AI加速器主要是为计算密集型的卷积而设计的。
但是，神经网络设计的一个趋势是采用更多的访存密集型的模块，例如Shuffle和SE。
它们在这些边缘加速器上成为了新的延迟瓶颈。
因此，在AI加速器上的解决内存墙（Memory Wall）的问题至关重要。

本文将如下安排：
节\ref{background}介绍了本文中使用的AI芯片和神经网络设计过程。
节\ref{methodology}阐述了本文提出的基准测试套件和严格的测试方法。
节todo分析了在每个设计维度上的神经网络行为。
节todo是关于神经网络设计空间的在真实情况下的案例研究。
