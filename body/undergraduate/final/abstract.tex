\cleardoublepage{}
\begin{center}
    \bfseries \zihao{3} 摘要
\end{center}
近年来，边缘设备上的应用程序越来越多地开始采用神经网络算法。
在这些设备进行推理的需求引出了大量新颖的AI芯片、框架以及高效的神经网络算法。
不幸的是，上述每个领域的演进都放大了高效算法设计和底层硬件之间的差距。
一个目标仅仅在减少数值计算量的神经网络算法很容易导致边缘端推理中的性能问题。

传统高效算法的设计依赖于专家对硬件特性的深刻理解。
然而，多样化的，并且大部分是闭源的AI芯片使得这样的做法变得非常困难。
为了在缺乏硬件知识的情况下使得高效算法的设计成为可能，本文提出一个基准测试套件：\sysname。
它涵盖了神经网络设计空间的主要维度，如组成模块、通道数、特征图/内核尺寸以及量化方法。
测量结果定量地反映了特定AI芯片在神经网络负载上表现出来的特性，并展示对神经网络设计的启示。

我们在七种不同的边缘端AI芯片上测量了此套件并展示了八个硬件特性相关的发现。
我们从这些发现中得出的主要结论是：
\begin{enumerate*}
    \item 卷积通道数和内核尺寸的增加不总是会导致延时的增加；
    \item 神经网络算子和组成模块的延时在不同硬件上的特性截然不同；
    \item 一些量化算法会减慢特定算子的推理速度以及会影响模型精度；
    \item 一些著名的轻量化模型仅仅在CPU上表现良好。
\end{enumerate*}

我们将这些发现应用在神经网络设计上并去除低效的配置后，
可以将MnasNet和MetaPruning的设计空间减少到原先的$32^{-1}$和$10^{-12}$。

\cleardoublepage{}
\begin{center}
    \bfseries \zihao{3} Abstract
\end{center}
Neural Network (NN) technology is increasingly deployed
into applications on edge devices. The demand of on-device
inference elicits vast novel AI chips, frameworks, and efficient NN algorithms.
Unfortunately, the fast evolvement in
each field magnifies the gap between efficient NN algorithm
design and underlying hardware. An NN algorithm simply
targeting a reduction in numerical computations for efficiency
can easily result in poor real-world performance.

Traditionally, efficient algorithm design relies on deep understanding of the hardware features. 
However, the unprecedented diverse, and mostly close-sourced, AI chips make
this practice impossible. To enable efficient NN design in
the absence of hardware knowledge, this paper proposes a
benchmark suite \sysname which covers major dimensions
of NN design space such as building blocks, channel number,
image/kernel size, and quantization. The profiling result can
quantitatively reflect the NN behaviours on target
hardware and reveal guidelines for NN design.

We profile this suite on seven industrial edge AI chips
and report eight NN behaviour findings. Major conclusions
from the findings are:
\begin{enumerate*}
    \item the increase in convolution channel number and kernel size
    does not always increase latency;
    \item the latency characteristics of NN operators and building
    blocks vary greatly on different hardware;
    \item quantization can largely slow down certain operators and ruin accuracy;
    \item some famous light-weight models only perform well on CPUs.
\end{enumerate*}
To apply these findings in NN design case studies and
eliminate the inefficient configurations, the design space size
for MnasNet and MetaPruning can be reduced by $32\times$ and
$10^{12}\times$, respectively.
