\newcounter{finding}[section]
\newenvironment{finding}[1][]{
    \refstepcounter{finding}\par
    \medskip\textbf{发现\thefinding#1} 
    \rmfamily}{\medskip}

\section{硬件行为分析}
\label{analysis}
本节分析了七种处理器上的基准测试结果，并展示了神经网络与硬件行为的发现。
本文会对不寻常的结果，基于硬件特性、框架实现和神经网络结构给出详尽的解释。
评估维度包括通道数（节\ref{analysis:channels}）、
模块类型、卷积内核尺寸、激活函数（节\ref{analysis:op block}）和
量化算法（节\ref{analysis:model quantization}）。

\subsection{通道数}
\label{analysis:channels}

\inputbody{final/analysis_misc/conv_pics}

通道数是对高效神经网络调优的重要参数。
基本的一个直觉是更大的通道数意味着更多计算数，因此有着更长的推理时间。
本节将展示通道数变化时真实的卷积延时响应。

图\ref{fig:conv_cin_fixed}展示了在各个硬件上变化$C_{out}$导致的卷积延时变化（固定$C_{in}$和其他超参数）。

\begin{finding}
    除了KPU之外，卷积延时随着输出通道数以台阶的规律增长。
\end{finding}

根据表\ref{tab:calculation}所示，当其他超参数固定时卷积的计算复杂度是$O(C_{out})$。
然而，测量获得的真实延时显示出了阶梯式的规律，其背后的根本原因是处理器的数据并行化。
这些处理器往往采用向量/矩阵计算单元来加速张量计算。
为了充分利用这些计算单元，上层的神经网络框架需要将数据分区，并将输入张量扩展到特定倍数，从而产生了台阶式的延时变化。

接下来，我们将以CPU、GPU和DSP的结果为例来阐述这一现象，因为它们的框架是开源的。
在其他闭源的处理器上，类似的解释很可能也适用。
注意，下文的每一个实验都只展示了一组超参数配置的数据。

CPU：TFLite v2.1使用\textit{im2col}\cite{vasudevan2017parallel}
将图片扩展成矩阵，并以矩阵乘法的方式以进行卷积。
如图\ref{fig:cpuconv}所示，每个卷积滤波器的内核被展开成左操作数中的一列，
相应特征图的区域则被展开为右操作数的一列。
之后TFLite则调用ruy\cite{ruy}矩阵运算库计算卷积。

ruy的矩阵计算针对ARMv8-A指令集做了优化。
该指令集包括了32个128位SIMD（即Neon）寄存器\cite{armisa}。
为了更好地利用这些寄存器，
ruy在计算FP32精度时将矩阵计算的最小单位设为$(8,1)\times (1,8)\rightarrow (8,8)$。
计算中，向量寄存器V16至V31被用作$(8,8)$大小的结果累加器。
寄存器V0至V3被用作加载左操作数$(8,1)$以及右操作数$(8,1)$。
为了适应这个最小的计算单位，两个输入矩阵都被填充到8的倍数。
因此，当其他超参数固定并且只有输出通道数增加时，
延时如图\ref{fig:conv_cin_fixed:cpu}所示地台阶式增长，并且台阶长度为8。

GPU：如节\ref{measurement}所述，我们利用TFLite的OpenCL后端在Adreno GPU上测试。
OpenCL的计算模型要求将索引空间划分成工作组（即CUDA中的块）\cite{lee2019device}。
TFLite的实现中，卷积算子的索引空间就是如图\ref{fig:gpuconv}所示的输出特征图。
一个工作组是GPU SIMT并行化中的基本计算模块。
工作组中的所有工作项执行同样的着色器代码，使用同样的工作组屏障。
因为输出特征图需要被填充到整数个工作组，
所以图\ref{fig:conv_cin_fixed:gpu}中的延时也呈现出阶梯式上升的规律。

然而，GPU延时的台阶宽度并不像CPU那样是固定的。
这是因为工作组尺寸和向量寄存器不同，它是由框架根据计算量设定的。
一个适当的工作组尺寸是由多种硬件配置，如Warp大小、计算单元数或共享内存大小共同决定的。
因此，TFLite使用了穷尽搜索策略来寻找工作组尺寸，这就导致了变化的台阶宽度。

DSP：Hexagon DSP 600系列的向量寄存器宽度为1024位。
为了配合这一点，Hexagon NNLib设计了Depth32\cite{nnlib}数据格式，
并将一个三维张量$(H,W,C)$的基本计算模块设为$(1,4,32)$的尺寸。
在INT8精度下，每个模块正好占用$32\times 4\times 8=1024$位。
为了适应这种数据格式，框架把所有张量的宽度一维填充为4的倍数，以及把通道数的维度填充为32的倍数。
之后两个基本块输入被打包成一对，并送入两个流中执行。
这就是图\ref{fig:conv_cin_fixed:dsp}显示，DSP的延时随着通道数增加有一个宽度为64的台阶的原因。

出于类似的原因，图\ref{fig:conv_cin_fixed}中的除KPU之外的其他处理器也表现出台阶的规律。
在KPU上通道数和延时之间则呈现出线性关系。
我们推测其原因是KPU对每个通道逐一顺序计算，因而不需要填充。

上述详细的分析再次说明，要理解神经网络行为，就需要对神经网络框架和硬件有深入的了解。
这证明我们用基准测试的方法来填补神经网络研究与底层系统之间空白的必要性和有效性。
基于上述的发现，神经网络设计空间可以只保留每个台阶的最大的通道数，
以达到可能的更高准确度（更多的OPs），而且没有额外的延时开销。

我们还评估了$C_{in}$维度上的卷积延时变化（由于篇幅限制没有画图）。
除了DSP和VPU外，由于在输入张量的维度上没有填充
（如图\ref{fig:cpuconv}和图\ref{fig:gpuconv}所示），
卷积延时随$C_{in}$线性变化。

\subsection{算子和模块}
\label{analysis:op block}
本节分析算子和模块的延时，
以回答高效神经网络设计过程中普遍关注的几个问题：
一个模块在不同处理器上（节\ref{analysis:op block:block with hardware}）、
一个模块在神经网络模型的不同层中（节\ref{analysis:op block:block with layer}）、
一个算子在不同的内核尺寸下（节\ref{analysis:op block:op with ksize}）、
一个算子带有不同激活函数的情况下（节\ref{analysis:op block:activation with hardware}），
延时表现是否相似或一样。

\subsubsection{不同硬件的模块延时}
\label{analysis:op block:block with hardware}

\inputbody{final/analysis_misc/hardware_diagrams}

神经网络是由重复的构建模块组成的。
选择合适的模块是设计中的重要考虑因素。
然而，神经网络设计者往往依靠OPs来寻找快速的模块，
并假定模块在每个处理器上的行为都是一样的。
本节将展示神经网络模块的真实延时特性。

我们选取四个模块作为例子，
并在图\ref{fig:latency_with_hardware}中列出它们的相对延时、相对OPs、相对mac。
``相对''指把它们的真实值除以在同一硬件上MobileNetV1模块的对应值以表现硬件行为的差异。
除了CPU外，其他加速器都有不支持的模块，例如，ShuffleNetV2模块不能在GPU和TPU上运行。
因此，他们的结果在图中是缺失的。

\begin{finding}
    每个神经网络模块在不同硬件上的延时表现差异极大，特别是对非卷积算子。
\end{finding}

主要原因有两个：
一是硬件之间内存和计算带宽差异很大。
特别是对AI加速器来说，高计算带宽使得内存墙的问题更加严重；
其次，AI处理器和相应框架对计算密集型的卷积或固定管线如卷积、批正则化、激活函数优化得很好，
但是对其他算子的支持却往往是层次不齐的。
我们将在下文中详细说明原因。

\paragraph{带宽的影响}
如图\ref{fig:latency_with_hardware}所示，模块延时和计算数仅仅在CPU上有着直接的关系。
图\ref{fig:roofline}解释了原因。
该图展示了理想情况下CPU和GPU的Roofline性能线，以及每个模块测试中的性能。
如果数据重用率位于拐点左侧，则性能的上限由内存带宽决定，
否则上限由算力限制。
注意，此图中我们使用的CPU和GPU是在同一块SoC上的，
它们使用同一块内存，所以Roofline中的内存受限的部分斜率是重合的。

在单核CPU上，由于内存和计算带宽相近，因而Roofline拐点处的数据重用率是相当低的。
因此，大部分模块都是计算受限的，它们的延时也与OPs直接相关。
相比之下，计算和内存带宽不匹配的问题在GPU上就相当严重。
数据重用率较小的模块，例如MobileNetV2+SE模块，就会变成内存受限，
这就对芯片的峰值性能有很大的影响。
内存带宽的问题在其他AI加速器更加严重。
例如，轻量级的MobileNetV2模块在CPU上可以比DenseNet模块快2.89倍，
在GPU上可以快2.13倍，但在TPU上只有1.25倍。

\paragraph{非卷积算子支持较弱}
硬件或框架实现都可能导致算子支持不佳。
正如节\ref{nn design and deployment}中所讨论的，
框架可以将相邻的算子融合，以避免中间结果写入内存。
这对于减少内存密集型算子的开销特别重要。
但是一般来说，这种融合是针对传统结构，如卷积、逐元素算子，
但框架常常无法将它应用到新的结构上。

在网络中添加SE模块被认为是可以使用极少代价就能提高模型准确度的有效方法。
然而，图\ref{fig:latency_with_hardware}显示，
在除了CPU的其他硬件上应用SE都会显著增加模块延时。
除了内存带宽的影响外，
一个重要的原因是，在SE（如图\ref{fig:blocks:se}）模块内，
全局池化或逐元素张量乘法的硬件/框架支持较弱。
在MobileNetV2+SE模块中，
虽然相比于DWConv+ReLU6（如图\ref{fig:blocks:mobilenetv2}），
乘法算子只占有了1/10的OPs，
然而在VPU上它却占据了68\%的模块推理延时，而DWConv+ReLU6只需要18\%的延时。
这可能是因为DWConv+ReLU6是由VPU内专用神经网络引擎执行的。
在DSP上，乘法算子也是模块的主要开销（78\%的延时），这很可能也是因为缺少框架实现的优化。

在GPU上，MobileNetV2+SE模块的主要开销是全局池化(71.7\%的延时)。
池化在网络中的位置导致其不能与其他算子融合以避免内存访问，
因此它引入了很大的开销。
SE模块中的全局池化和乘法算子都不能在KPU上运行，它们会退回到SoC上的RISC-V CPU执行。
在TPU上异常高的SE延时也表明TPU工具链对这些算子的支持较弱。

ShuffleNetV2模块在VPU和NPU上高延时的原因在于Shuffle（\ref{fig:blocks:shufflenetv2}）算子。
该算子占据了超过50\%的模块延时。
框架通常也无法将其与其他算子融合在一起，这就形成了延时瓶颈。

总结一下，由于硬件特性或框架实现的原因，模块在每个处理器上的延时表现是不同的。
我们的基准测试可以抽象出这些细节，并为神经网络设计找到高效的模块。

\subsubsection{不同层的模块延时}
\label{analysis:op block:block with layer}

\subsubsection{不同内核尺寸的算子延时}
\label{analysis:op block:op with ksize}

\subsubsection{不同硬件的激活函数延时}
\label{analysis:op block:activation with hardware}

\subsection{模型与量化算法}
\label{analysis:model quantization}