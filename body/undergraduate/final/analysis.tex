\newcounter{finding}[section]
\newenvironment{finding}[1][]{
    \refstepcounter{finding}\par
    \medskip\textbf{发现\thefinding#1} 
    \rmfamily}{\medskip}

\section{硬件行为分析}
\label{analysis}
本节分析了七种处理器上的基准测试结果，并展示了神经网络与硬件行为的发现。
本文会对不寻常的结果，基于硬件特性、框架实现和神经网络结构给出详尽的解释。
评估维度包括通道数（节\ref{analysis:channels}）、
模块类型、卷积内核尺寸、激活函数（节\ref{analysis:op block}）和
量化算法（节\ref{analysis:model quantization}）。

\subsection{通道数}
\label{analysis:channels}

\inputbody{final/analysis_misc/conv_pics}

通道数是对高效神经网络调优的重要参数。
基本的一个直觉是更大的通道数意味着更多计算数，因此有着更长的推理时间。
本节将展示通道数变化时真实的卷积延时响应。

图\ref{fig:conv_cin_fixed}展示了在各个硬件上变化$C_{out}$导致的卷积延时变化（固定$C_{in}$和其他超参数）。

\begin{finding}
    除了KPU之外，卷积延时随着输出通道数以台阶的规律增长。
\end{finding}

根据表\ref{tab:calculation}所示，当其他超参数固定时卷积的计算复杂度是$O(C_{out})$。
然而，测量获得的真实延时显示出了阶梯式的规律，其背后的根本原因是处理器的数据并行化。
这些处理器往往采用向量/矩阵计算单元来加速张量计算。
为了充分利用这些计算单元，上层的神经网络框架需要将数据分区，并将输入张量扩展到特定倍数，从而产生了台阶式的延时变化。

接下来，我们将以CPU、GPU和DSP的结果为例来阐述这一现象，因为它们的框架是开源的。
在其他闭源的处理器上，类似的解释很可能也适用。
注意，下文的每一个实验都只展示了一组超参数配置的数据。

CPU：TFLite v2.1使用\textit{im2col}\cite{vasudevan2017parallel}
将图片扩展成矩阵，并以矩阵乘法的方式以进行卷积。
如图\ref{fig:cpuconv}所示，每个卷积滤波器的内核被展开成左操作数中的一列，
相应特征图的区域则被展开为右操作数的一列。
之后TFLite则调用ruy\cite{ruy}矩阵运算库计算卷积。

ruy的矩阵计算针对ARMv8-A指令集做了优化。
该指令集包括了32个128位SIMD（即Neon）寄存器\cite{armisa}。
为了更好地利用这些寄存器，
ruy在计算FP32精度时将矩阵计算的最小单位设为$(8,1)\times (1,8)\rightarrow (8,8)$。
计算中，向量寄存器V16至V31被用作$(8,8)$大小的结果累加器。
寄存器V0至V3被用作加载左操作数$(8,1)$以及右操作数$(8,1)$。
为了适应这个最小的计算单位，两个输入矩阵都被填充到8的倍数。
因此，当其他超参数固定并且只有输出通道数增加时，
延时如图\ref{fig:conv_cin_fixed:cpu}所示地台阶式增长，并且台阶长度为8。

GPU：如节\ref{measurement}所述，我们利用TFLite的OpenCL后端在Adreno GPU上测试。
OpenCL的计算模型要求将索引空间划分成工作组（即CUDA中的块）\cite{lee2019device}。
TFLite的实现中，卷积算子的索引空间就是如图\ref{fig:gpuconv}所示的输出特征图。
一个工作组是GPU SIMT并行化中的基本计算模块。
工作组中的所有工作项执行同样的着色器代码，使用同样的工作组屏障。
因为输出特征图需要被填充到整数个工作组，
所以图\ref{fig:conv_cin_fixed:gpu}中的延时也呈现出阶梯式上升的规律。

然而，GPU延时的台阶宽度并不像CPU那样是固定的。
这是因为工作组尺寸和向量寄存器不同，它是由框架根据计算量设定的。
一个适当的工作组尺寸是由多种硬件配置，如Warp大小、计算单元数或共享内存大小共同决定的。
因此，TFLite使用了穷尽搜索策略来寻找工作组尺寸，这就导致了变化的台阶宽度。

DSP：Hexagon DSP 600系列的向量寄存器宽度为1024位。
为了配合这一点，Hexagon NNLib设计了Depth32\cite{nnlib}数据格式，
并将一个三维张量$(H,W,C)$的基本计算模块设为$(1,4,32)$的尺寸。
在INT8精度下，每个模块正好占用$32\times 4\times 8=1024$位。
为了适应这种数据格式，框架把所有张量的宽度一维填充为4的倍数，以及把通道数的维度填充为32的倍数。
之后两个基本块输入被打包成一对，并送入两个流中执行。
这就是图\ref{fig:conv_cin_fixed:dsp}显示，DSP的延时随着通道数增加有一个宽度为64的台阶的原因。

出于类似的原因，图\ref{fig:conv_cin_fixed}中的除KPU之外的其他处理器也表现出台阶的规律。
在KPU上通道数和延时之间则呈现出线性关系。
我们推测其原因是KPU对每个通道逐一顺序计算，因而不需要填充。

上述详细的分析再次说明，要理解神经网络行为，就需要对神经网络框架和硬件有深入的了解。
这证明我们用基准测试的方法来填补神经网络研究与底层系统之间空白的必要性和有效性。
基于上述的发现，神经网络设计空间可以只保留每个台阶的最大的通道数，
以达到可能的更高准确率（更多的OPs），而且没有额外的延时开销。

我们还评估了$C_{in}$维度上的卷积延时变化（由于篇幅限制没有画图）。
除了DSP和VPU外，由于在输入张量的维度上没有填充
（如图\ref{fig:cpuconv}和图\ref{fig:gpuconv}所示），
卷积延时随$C_{in}$线性变化。

\subsection{算子和模块}
\label{analysis:op block}
本节分析算子和模块的延时，
以回答高效神经网络设计过程中普遍关注的几个问题：
一个模块在不同处理器上（节\ref{analysis:op block:block with hardware}）、
一个模块在神经网络模型的不同层中（节\ref{analysis:op block:block with layer}）、
一个算子在不同的内核尺寸下（节\ref{analysis:op block:op with ksize}）、
一个算子带有不同激活函数的情况下（节\ref{analysis:op block:activation with hardware}），
延时表现是否相似或一样。

\subsubsection{不同硬件的模块延时}
\label{analysis:op block:block with hardware}

\inputbody{final/analysis_misc/hardware_diagrams}

神经网络是由重复的构建模块组成的。
选择合适的模块是设计中的重要考虑因素。
然而，神经网络设计者往往依靠OPs来寻找快速的模块，
并假定模块在每个处理器上的行为都是一样的。
本节将展示神经网络模块的真实延时特性。

我们选取四个模块作为例子，
并在图\ref{fig:latency_with_hardware}中列出它们的相对延时、相对OPs、相对mac。
``相对''指把它们的真实值除以在同一硬件上MobileNetV1模块的对应值以表现硬件行为的差异。
除了CPU外，其他加速器都有不支持的模块，例如，ShuffleNetV2模块不能在GPU和TPU上运行。
因此，他们的结果在图中是缺失的。

\begin{finding}
    每个神经网络模块在不同硬件上的延时表现差异极大，特别是对非卷积算子。
\end{finding}

主要原因有两个：
一是硬件之间内存和计算带宽差异很大。
特别是对AI加速器来说，高计算带宽使得内存墙的问题更加严重；
其次，AI处理器和相应框架对计算密集型的卷积或固定管线如卷积+批归一化+激活函数优化得很好，
但是对其他算子的支持却往往是层次不齐的。
我们将在下文中详细说明原因。

\paragraph{带宽的影响}
如图\ref{fig:latency_with_hardware}所示，模块延时和计算数仅仅在CPU上有着直接的关系。
图\ref{fig:roofline}解释了原因。
该图展示了理想情况下CPU和GPU的Roofline性能线，以及每个模块测试中的性能。
如果数据重用率位于拐点左侧，则性能的上限由内存带宽决定，
否则上限由算力限制。
注意，此图中我们使用的CPU和GPU是在同一块SoC上的，
它们使用同一块内存，所以Roofline中的内存受限的部分斜率是重合的。

在单核CPU上，由于内存和计算带宽相近，因而Roofline拐点处的数据重用率是相当低的。
因此，大部分模块都是计算受限的，它们的延时也与OPs直接相关。
相比之下，计算和内存带宽不匹配的问题在GPU上就相当严重。
数据重用率较小的模块，例如MobileNetV2+SE模块，就会变成内存受限，
这就对芯片的峰值性能有很大的影响。
内存带宽的问题在其他AI加速器更加严重。
例如，轻量级的MobileNetV2模块在CPU上可以比DenseNet模块快2.89倍，
在GPU上可以快2.13倍，但在TPU上只有1.25倍。

\paragraph{非卷积算子支持较弱}
硬件或框架实现都可能导致算子支持不佳。
正如节\ref{nn design and deployment}中所讨论的，
框架可以将相邻的算子融合，以避免中间结果写入内存。
这对于减少内存密集型算子的开销特别重要。
但是一般来说，这种融合是针对传统结构，如卷积、逐元素算子，
但框架常常无法将它应用到新的结构上。

在网络中添加SE模块被认为是可以使用极少代价就能提高模型准确率的有效方法。
然而，图\ref{fig:latency_with_hardware}显示，
在除了CPU的其他硬件上应用SE都会显著增加模块延时。
除了内存带宽的影响外，
一个重要的原因是，在SE（如图\ref{fig:blocks:se}）模块内，
全局池化或逐元素张量乘法的硬件/框架支持较弱。
在MobileNetV2+SE模块中，
虽然相比于逐通道卷积+ReLU6（如图\ref{fig:blocks:mobilenetv2}），
乘法算子只占有了1/10的OPs，
然而在VPU上它却占据了68\%的模块推理延时，而逐通道卷积+ReLU6只需要18\%的延时。
这可能是因为逐通道卷积+ReLU6是由VPU内专用神经网络引擎执行的。
在DSP上，乘法算子也是模块的主要开销（78\%的延时），这很可能也是因为缺少框架实现的优化。

在GPU上，MobileNetV2+SE模块的主要开销是全局池化(71.7\%的延时)。
池化在网络中的位置导致其不能与其他算子融合以避免内存访问，
因此它引入了很大的开销。
SE模块中的全局池化和乘法算子都不能在KPU上运行，它们会退回到SoC上的RISC-V CPU执行。
在TPU上异常高的SE延时也表明TPU工具链对这些算子的支持较弱。

ShuffleNetV2模块在VPU和NPU上高延时的原因在于Shuffle（\ref{fig:blocks:shufflenetv2}）算子。
该算子占据了超过50\%的模块延时。
框架通常也无法将其与其他算子融合在一起，这就形成了延时瓶颈。

总结一下，由于硬件特性或框架实现的原因，模块在每个处理器上的延时表现是不同的。
我们的基准测试可以抽象出这些细节，并为神经网络设计找到高效的模块。

\subsubsection{不同层的模块延时}
\label{analysis:op block:block with layer}

\inputbody{final/analysis_misc/layer_diagrams}

目前的神经网络设计通常采用在每一层中重复使用一种模块的方法。
我们在不同的层（即不同的特征图尺寸）中，和在每个处理器上都测量了模块的延时，
并在图\ref{fig:block_latency_cpu}中展示了CPU的结果。
在测量中，特征图尺寸和通道数遵循了``特征图边长减半，通道数加倍''的规则。

\begin{finding}
    在CPU上，没有一个模块总是在每一层中都最好；在其他加速器上，MobileNetV1模块总是最快的。
\end{finding}

如图\ref{fig:block_latency_cpu}所示，
随着层数加深（特征图尺寸变小，通道数增加），模块的延时响应在不断变化。
这是因为计算和访存复杂度随层数的变化在不同模块间是不一样的。
根据表\ref{tab:calculation}中的公式，对于卷积和分组卷积，
假定$C_{in}=C_{out}$，该层的OPs为$H^2C_{in}^2K^2$，
则下一层具有相同的OPs（$(1/2H)^2\times (2C_{in})^2K^2$）。
而对于逐通道卷积，OPs相对于上一层减半（$(1/2H)^2\times 2C_{in}K^2$）。
这对逐元素算子也成立。
本文用输入、输出张量和内核的尺寸之和来估算一个算子访存数（mac）。
随着层数加深，输入、输出张量的尺寸总是减半（$(1/2H)^2\times 2C_{in}$）。
然而，卷积内核的尺寸额外增加了3倍（$(2 C_{in})^2K^2$），
逐通道卷积的内核尺寸额外增加了1倍（$2 C_{in}K^2$）。

对于由卷积和逐通道卷积组成的MobileNet模块，
直到$14\times 14$层，输入/输出张量尺寸的减少量超过了内核尺寸的增加量。
如图\ref{fig:block_latency_cpu}所示，
其中$14\times 14$层有着最小的mac和最小的延时。

对于由分组卷积、逐通道卷积和通道重排算子组成的ShuffleNetV1模块，
内核尺寸远比输入/输出张量尺寸小。
因而总mac一直在成倍减少，其延时也不断降低。
ShuffleNetV2模块用普通卷积代替了分组卷积，所以mac的减少量比ShuffleNetV1模块小。

总的来说，MobileNetV1模块在前两层最快，
但之后层中ShuffleNetV1/V2模块更快。
因为其他AI加速器对非卷积算子如通道重排的支持不佳（如节\ref{analysis:op block:block with hardware}所述），
所以MobileNetV1模块在每层上始终是最快的。

\subsubsection{不同内核尺寸的算子延时}
\label{analysis:op block:op with ksize}

\inputbody{final/analysis_misc/ksize_diagrams}

一般来说，卷积内核尺寸（$K$）的选择是$1\times 1$、$3\times 3$、$5\times 5$和$7\times 7$。
对于边缘设备来说，内核尺寸通常会在卷积中选择$1\times 1$，在逐通道卷积中选择$3\times 3$，
以达到减少延时的效果（如图\ref{fig:blocks}）。
如图\ref{fig:latency_with_ksize}所示，我们在每个处理器上评估了不同内核尺寸的延时响应。

\begin{finding}
    在AI加速器上，卷积（除TPU上的逐通道卷积）延时随内核尺寸增长的幅度相对CPU、GPU要小得多。
\end{finding}

当其他超参数（如$C_{in}$、$C_{out}$）固定时，
卷积和逐通道卷积的计算复杂度、内核访存复杂度是$O(K^2)$。
对于CPU和GPU上的卷积（如图\ref{fig:latency_with_ksize}a），
其延时随内核尺寸的增长，快于OPs的增长。
这是因为增加的数据量会提高缓存的容量失效率（Capacity Miss）（CPU上有2M的L3缓存）。
另一个原因是，$1\times 1$卷积在目前的TFLite中的有着和其他内核尺寸不同的优化实现。
相比之下，在其他AI加速器上，延时随内核大小的增长要小很多（尤其是$K=3$时的NPU和TPU）。
这是合理的，因为这些加速器与其框架是主要为了加速卷积而设计的。
例如，TPU具有用于内核数据预取的片上权重FIFO，
而DSP的神经网络库针对每个内核大小有定制化的汇编优化。

对于逐通道卷积来说，其延时增加看起来更加温和，
特别是在DSP、VPU和NPU上（注意图中标注的相对于$K=3$的延时）。

\subsubsection{不同硬件的激活函数延时}
\label{analysis:op block:activation with hardware}

\inputbody{final/analysis_misc/activation_diagrams}

激活函数作为一种逐元素算子，是内存密集型的。
为了减少延时，框架通常将其与卷积、逐通道卷积融合在一起。
ReLU和ReLU6作为传统的激活函数，在每个框架中都获得了最佳的融合支持，因此其开销往往可以忽略不计。
近年来，Swish被提出用于取代ReLU，以提高准确率。
为了降低Swish的计算复杂度，HardSwish用近似的方法来计算Swish。
我们用逐通道卷积连接图\ref{fig:activation_latency}所示的激活函数，
来评估这些激活函数的效率。

\begin{finding}
    激活函数对推理延时有很大的影响。
    HardSwish仅仅在CPU和GPU上效率很高。
    ReLU和ReLU6在所有硬件上都非常高效。
\end{finding}

五种激活函数的延时特性在不同的硬件上有很大差异。
考虑到其计算复杂度，它们延时应当遵循这样的顺序：ReLU < ReLU6 < HardSwish < Sigmoid < Swish。
然而，只有在CPU上，激活函数的延时与OPs保持一致。
在其他处理器上的不同表现，是由算子融合的实现不同或硬件特征的差异而导致的。

在KPU上，每个激活函数都是由一个分段三次样条拟合的。
所有的激活函数都会被同等地嵌入卷积+批归一化+激活函数的固定硬件管线。
因此，四种支持的激活函数在KPU上表现出了类似的性能。
Swish和HardSwish在GPU或VPU上的高延时是由算子融合失败导致的。
HardSwish在NPU上的推理速度非常慢，因为它被分割成多个未融合的算子来执行。

\subsection{模型与量化算法}
\label{analysis:model quantization}

\inputbody{final/analysis_misc/quant_diagrams_and_tab}

INT8或FP16量化被认为是一种有效的在精度损失很小的情况下加速模型推理的方法。
因此，AI加速器往往是为INT8设计的。
表\ref{tab:quantization accuracy latency}比较了支持不同量化策略的三种处理器。
图\ref{fig:model accuracy}展示了基准测试的模型在支持的处理器上的延时和准确率。

\begin{finding}
    INT8量化在NPU上具有很高的加速比，但在CPU上可能导致速度变慢。
\end{finding}

在CPU上，SIMD单元同时支持FP32和INT8计算。
理想情况下，除了降低为1/4的访存开销，INT8也应该达到4倍的加速比。
然而，表\ref{tab:quantization accuracy latency}中展示的加速比是低于预期的。
原因之一当然是计算库的实现，完全利用好SIMD单元是困难的。
另一个原因是每个算子的输出张量都需要被复量化（Requantize）。
\textbf{INT8复量化对数据重用率低的算子来说是开销高昂的。}

为了避免算数溢出，INT8算子的输出张量类型是INT32。
因此，框架需要复量化才能将INT32张量映射到INT8\cite{krishnamoorthi2018quantizing}的范围。
对于高数据重用率的算子如卷积，该代价与INT8的加速相比可以忽略不记。
但在低数据重用率的算子上，这个代价可能会超过量化的加速。
例如，测量结果表明，在CPU上INT8加法算子比FP32版本慢达5倍。
因此，带有SE模块（低数据重用率）的MobileNetV3仅仅用INT8量化达到了1.54的加速比。
在ShuffleNetV1，量化甚至增加了延时。
而运算量集中于卷积的模型如ResNet比基于逐通道卷积的MobileNet系列有着更高的INT8量化加速比。

在GPU上，FP16算子的输出张量仍然是FP16，所以没有复量化开销。

在NPU上，除了MobileNetV3之外UINT8和DFP8相比FP16有着超过11的加速比。
这是因为NPU有着大量的8位卷积神经网络计算单元。
这些计算单元可能不能很好地支持SE中的全局池化和逐元素乘法算子，
因而MobileNetV3加速比较低。

DFP16相对FP16有着1.31到3的加速比，因为DFP16比FP16计算消耗了更少的周期数。

模型的准确性超出了本文的范畴。
然而，一些量化模型显示出了意想不到的准确率损失。

\begin{finding}
    不是所有的模型都适合应用INT8量化；
    一些AI加速器上的量化算法不是对所有的模型都鲁棒。
\end{finding}

表\ref{tab:quantization accuracy latency}和图\ref{fig:model accuracy}
展示了MobileNetV3模型的INT8量化会导致在每个支持的处理器上都会有大幅度的准确率下降，
比如CPU（-58.7\%）、NPU（-75.4\%）和DSP（-75.5\%）。
一个原因是INT8的数值范围不足以满足SE模块\cite{google_quantization}。

此外，图\ref{fig:model accuracy}显示了许多其他模型的INT8量化
都会导致在TPU和DSP上的准确率下降。
例如MnasNet-A1在TPU上的Top-1准确率从73.4\%下降到8.7\%。
同样地，MobileNetsV1/V2在DSP上的INT8量化上也经历了显著的准确率损失。
这是因为这些模型中的ReLU6和Swish激活函数不能被TPU和DSP上的量化算法
很好地支持\cite{google_quantization, sheng2018quantization}。

\begin{finding}
    考虑到通用支持性、准确率和延时，CPU依旧是个不错的选择。
\end{finding}

除CPU外，所有其他处理器都有不支持或性能优化不佳的算子/模块/模型。
如图\ref{fig:model accuracy}所示，
著名的轻量级网络MobileNetV3的CPU推理是各类端侧处理器中最快的，
尽管其他加速器宣称的峰值性能可能比CPU高出数百倍。
注意该结果是只在一个单核ARM CPU上使用FP32精度完成的。
但是其他加速器依然在特定模型上有卓越的加速比，
例如TPU上的InceptionV1模型和NPU上的ResNetV1模型，
它们相比CPU都有高达25的加速比。
在其他模型上，加速比则低得多。