\section{引言}
\subsection{背景}
近年来，深度神经网络已经广泛地部署在了移动设备上，被用于目标检测、人像美颜等。
这些神经网络通常在服务端被训练，并通过应用的安装包部署到边缘端推理。
相比较于云端的推理，设备端推理的好处在于隐私保护、支持离线应用、低延时等。
因此，高效地在资源受限的边缘端推理神经网络成为了重要的话题。

为了完成这样的目标，产研两界花费了巨大的精力在底层硬件和上层软件上，包括神经网络设计。
对硬件来说，除了CPU和GPU，工业界设计了多样的低功耗高算力的边缘神经网络处理器，比如
Google Edge TPU（张量处理单元）\cite{edgetpu}、
Intel Movidius Myrid X VPU（视觉处理单元）\cite{myriad}
和福州瑞芯微开发的RK1808 NPU（神经处理单元）\cite{rk3399pro}等。
在软件层面上，有来自Google的广泛使用的开源TensorFlow Lite，Facebook推广的PyTorch Mobile，
和其他诸多硬件厂商的闭源推理引擎。
就比如高通为在骁龙SoC上推理神经网络开发的SNPE（高通神经处理引擎）\cite{snpe}。
在神经网络层面上，研究员们已经提出了许多神经网络加速技术，来减少神经网络模型的计算复杂度使其能在移动端推理。

\subsection{存在的问题与挑战}
然而，神经网络设计和底层的框架、硬件之间是有差距的。
神经网络设计者通常着重于提高网络的精度，而不是理解特定硬件以减少在其上运行的延时。
因此，在高效神经网络设计方面就存在一些不合理：
其中一点是总计算操作数，比如FLOPs（浮点操作数）是延时的代理衡量标准。
有许多工作的目标在降低这个指标以获得在精度和延时之间的平衡。
然而，降低的计算量不总是意味着降低的延时。
就比如在FLOPs指标上比MobileNet V2\cite{sandler2018mobilenetv2}高88\%的
MobileNet V1\cite{howard2017mobilenets}在Edge TPU上具有比V2模型更低的延时。
另外一点不合理认知是在一个处理器上能高效运行的模型也能在另一种处理器上具有较低延时。
根据我们的测试，在高通855 SoC处理器上比MobileNet V3 Large\cite{howard2019searching}
相较于V2模型有1.26的加速比，然而在RK1808计算棒的半精度推理上，前者却比后者慢44.2\%。

一些神经网络设计的工作\cite{tan2019mnasnet,dai2019chamnet,yang2018netadapt}
注意到了这些不合理之处，并使用真实的在硬件上测量出的延时，在全搜索空间上搜索高效的网络结构。
然而考虑到算子、单元的类型和网络中每层的超参数配置构成了很大的搜索空间，
在每个硬件上对每个模型测量变成了极其困难的事情。
那么，如何填补在网络设计和硬件之间的差距，变成了一个关键问题。

要解决这个问题，一大挑战是人工智能处理器和神经网络框架一直在频繁的开发与更新过程中。
更糟糕的是，这些处理器中的许多是完全黑盒的，只能配合厂商提供的闭源推理引擎和编译器使用。
因此，上述人工智能处理器和框架的组合使得高效神经网络设计面临着巨大的复杂性。
在这种情况下，传统的方法，如对延时建模以避免实际测量，或由硬件专家手动设计优化算法，是不合理的。

\subsection{研究方法与预期贡献}
本项目将使用如下的方法展开研究，以应对上述的挑战：
\begin{itemize}
    \item 我们构建了一个基准评测套件，并在特定硬件上测试来为神经网络设计展示硬件特性；
    \item 评测套件涵盖了神经网络设计中的常见配置，比如通道数、算子或单元类型、模型精度等；
    \item 评测在多种不同架构的硬件上进行，比如CPU、GPU以及其他有代表性的ASIC。
\end{itemize}

我们预期能做出如下的贡献：
\begin{itemize}
    \item 基准评测套件以及测试的方法；
    \item 系统的对一系列有代表性硬件的研究；
    \item 在测试过程中发现的新启示，如：
    \begin{itemize}
        \item 在特定硬件上，通道数和当层的延时并不是线性的关系，而是阶梯状上升的关系；
        \item FLOPs较低的算子不总是在各类硬件上都具有较低的延时，以及可能的原因；
        \item 低精度的网络不总是能推理得更快，以及可能的原因。
    \end{itemize}
    \item 展示如何利用上述的启示辅助神经网络设计的使用案例。比如如何缩小神经网络的搜索空间等。
\end{itemize}