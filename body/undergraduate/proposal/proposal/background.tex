\section{问题提出的背景}

\subsection{背景介绍}

在过去的十年里，深度学习已经逐渐成为了最主流的机器学习算法，并在图像处理、机器翻译、语音识别等诸多应用领域内取得了
巨大的成功。在深度学习算法中，深度神经网络凭借庞大而极深的模型，通过训练若干GB甚至若干TB的数据来达到在某些领域卓越
的精确度。然而这样的模型精度带来的推理计算复杂度代价是当下许多移动端、嵌入式设备无法承受的。那么针对移动边缘端设备
制定合适的深度神经网络就格外重要。

在以往的研究中，Xception、ResNeXt、MobileNet系列、ShuffleNet系列都针对边缘端设备内存较小、计算能力相对服务端较弱
的特性手工设计计算量相对较小的网络构造基本单元（分组卷积，深度可分离卷积等）。这些网络都在维持了网络的表达能力和精度
的基础上，极大地降低了网络的FLOPs（浮点运算操作数），从理论上降低了网络的计算复杂度。而ShuffleNet V2在考虑计算复杂
度的同时也考虑了MAC（内存访问代价），分析之后摒弃了分组卷积，并取得了在近似FLOPs的情况下，GPU推理延时远小于先前
SOTA（State Of The Art）模型的优秀成果。同时，这也指示了FLOPs作为一个理论指标，并不能完全用于评价特定模型在特定
AI硬件上的执行效率。

除了网络结构的突破之外，近年来在移动边缘端设备上的神经网络推理平台、AI加速芯片都有卓越的进步。TensorFlow有配套的专为
移动端推理优化的TensorFlow Lite；PyTorch推出了Pytorch Mobile；近年来国内的端侧推理引擎有腾讯的NCNN，
来自小米的MACE框架，百度的PaddlePaddle Lite，阿里巴巴的MNN等。除了软件之外，硬件平台也是百花齐放、百家争鸣：
高通骁龙SoC上除了ARM Cortex CPU、Adreno GPU之外还有利用VLIW（超长指令）的适合图像处理和神经网络计算的Hexagon DSP
（电子信号处理器）；华为在2017年底推出了的Mate 10上搭载了寒武纪提供的全球第一款商用NPU（神经网络处理单元）Cambricon-1A，
这款芯片神经网络推理的性能与功耗比超过当年的旗舰CPU、GPU；最近Google推出的EdgeTPU，更是在8-bit推理MobileNet V2上达到了
接近400帧的速度；除此之外，Intel推出的Movidius计算棒，Rockchip（福州瑞芯微）推出的RK1808计算棒等，都很可能在
物联网、智慧城市等领域找到合适的落地场景大放异彩。

% \subsubsection{项目提出的原因}

\section{本研究的意义和目的}

近年来不论是在软件上还是在硬件上，神经网络推理平台表现出多样化，异构化的趋势。不同的硬件有截然不同的特性，在一种芯片
上能高效执行的网络不一定能很好地利用另一种芯片的硬件资源。如此看来，利用单一化的FLOPs作为衡量模型计算复杂度的指标来
指导设计神经网络是不合适的。那么针对多样化的软硬件寻找对应的合适的方法，设计高效的合适的神经网络就成为了重要的话题。
然而，这样“因地制宜”的过程往往会遭遇许多困难，比如：硬件厂商通常不愿意提供详细的硬件设计信息；许多边缘端AI芯片厂商
提供的神经网络编译器、推理框架通常也不会开源。这就给分析特定神经网络在特定AI芯片上的执行效率造成了极大地阻碍，也
进一步阻碍了为特定的AI芯片设计合适的神经网络。

本研究就在这样困难的背景下产生。本研究计划通过benchmark的方式，测量多种神经网络基本单元在不同硬件的延时、能耗、
效率（FLOPs/latency）表现，对比基本单元的理论复杂度，并结合已有的资料（开源代码，可信的硬件架构信息等），分析
理论复杂度和现实中延时的差别，给“为异构硬件设计合适的神经网络”的目标提供实验数据和扎实的分析结果。

测量是理解和优化的关键。

