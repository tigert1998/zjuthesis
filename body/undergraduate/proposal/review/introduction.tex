\section{引言}
近年来，深度学习被应用在了各种不同的应用领域上，比如图片分类、目标识别、语音识别与合成等。
在许多的工作中，研究员们构建了庞大的神经网络来尝试解决一个领域内的问题。
这些网络包含了数以百万计甚至十亿计的参数量，并依赖于服务端GPU/TPU来进行推理。
例如说Vgg-16\cite{simonyan2014very}包含了138兆的参数；
Bert-Base\cite{devlin2018bert}则包含了110兆的参数，32位浮点模型就要占据按GB计的存储空间。
这些模型也在特定的任务上卓有成效：
AlexNet\cite{krizhevsky2012imagenet}将2012 ImageNet\cite{deng2009imagenet}图片分类竞赛的Top-5错误率从28.2\%降低
到17.0\%；GPT-2\cite{radford2019language}在WikiText等多个数据集上都刷新了成绩标杆。

然而当神经网络的层数与算子数越多，模型推理对硬件的要求就水涨船高，模型的部署就变得代价高昂。
在延时敏感的场景下，比如摄像背景虚化、虚拟现实、增强现实等，推理这些模型就更加困难。
因而降低它们的参数量和计算复杂度就愈发重要。

另外，随着人们越来越关注私人设备上数据的保密性，人们更加希望数据能在不需要被上传到云端的情况下被处理；
随着边缘端计算设备的算力越来越强，移动应用厂商逐渐更加愿意将把机器学习算法部署在端侧以减少推理的延时开销和服务器的维护成本。
以这样的需求为背景，在硬件资源受限的边缘端部署神经网络就变成了富有机遇与挑战的研究话题。
精简的模型设计可以在上述的背景下发挥作用。
比如Inception V4\cite{szegedy2017inception}使用了$2.46\times 10^{10}$的浮点操作数，
在高通骁龙855 SoC处理器上推理要花费715毫秒；
而去掉了许多冗余计算量的MobileNet V2\cite{sandler2018mobilenetv2}
在$6.0\times 10^8$浮点操作数和同硬件49毫秒延时的情况下，ImageNet测试集精度达到了仅低10\%的效果。
再比如TensorFlow Lite提供的8-bit量化在MobileNet V2上可以达到在精度仅低0.6\%的情况下，
将延时降低50\%，模型大小降低75\%。
类似的取舍使得边缘端智能推理在嵌入式设备、FPGA等资源受限的场景下成为可能。

本文将回顾这些的压缩、加速模型推理的方法，以及近年来边缘端硬件AI加速器的相关研究进展。
本文将这些压缩加速方法分为几类：量化、知识蒸馏、轻量化模型设计。
另外本文会简要介绍近年来主流的边缘端神经网络推理设备，以及对这些处理器做基准测试的相关研究。