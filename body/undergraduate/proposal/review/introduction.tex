\section{引言}
近年来，深度学习被应用在了各种不同的应用领域上，比如图片分类、目标识别、语音识别与合成等。
在许多的工作中，研究员们构建了庞大的神经网络来尝试解决一个领域内的问题。
这些网络包含了数以百万计甚至十亿计的参数量，并依赖于服务端GPU/TPU来进行推理。
例如说Vgg-16\cite{simonyan2014very}包含了138兆的参数；
Bert-Base\cite{devlin2018bert}则包含了110兆的参数，32位浮点模型就要占据按GB计的存储空间。
这些模型也在特定的任务上卓有成效：
AlexNet\cite{krizhevsky2012imagenet}将2012 ImageNet\cite{deng2009imagenet}图片分类竞赛的Top-5错误率从28.2\%降低
到17.0\%；GPT-2\cite{radford2019language}在WikiText等多个数据集上都刷新了成绩标杆。

随着神经网络的能力越来越强，许多公司都更加希望赋予他们的移动端应用以AI的能力，比如虚拟现实、增强现实等。
相较于在云端的神经网络推理，离线的边缘端推理因如下的优势在特定场景下更受用户和公司的青睐：
\begin{enumerate*}
    \item 支持应用的离线使用；
    \item 用户数据不需要被发向云端因而能更好地保护用户的隐私；
    \item 去除了网络延时的干扰，应用将更加稳定；
    \item 节省服务器的维护开销。
\end{enumerate*}
这些离线的神经网络工作负载依赖于边缘端芯片执行计算，比如移动端CPU、GPU等。
除此之外，许多硬件公司还为上述的需求而定制了边缘端的AI芯片。
比如Google Pixel 4上搭载了基于脉动阵列架构的Edge TPU\cite{edgetpu}，
华为在麒麟990上搭载了达芬奇架构的NPU。

尽管这些芯片中的许多有不小的算力，然而在嵌入式设备上的它们依旧受限于能耗等条件的限制，
将具有大量参数的神经网络直接部署在它们上面是不合适的。
过往有许多工作，目标在根据特定芯片硬件资源受限等特性，精简神经网络模型，使其适合边缘端硬件。
比如Inception V4\cite{szegedy2017inception}使用了$2.46\times 10^{10}$的浮点操作数，
在高通骁龙855 SoC处理器上推理要花费715毫秒；
而去掉了许多冗余计算量的MobileNet V2\cite{sandler2018mobilenetv2}
在$6.0\times 10^8$浮点操作数和同硬件49毫秒延时的情况下，ImageNet测试集精度达到了仅低10\%的效果。
再比如TensorFlow Lite提供的8-bit量化在MobileNet V2上可以达到在精度仅低0.6\%的情况下，
将延时降低50\%，模型大小降低75\%。

本文将回顾主流的边缘端神经网络推理设备，以及对这些处理器做基准测试的相关研究。
之后，本文将简述近年来面向边缘端硬件的模型设计、优化方法。
本文将这些设计、优化方法分为几类：轻量化模型设计、量化、知识蒸馏。
