\section{知识蒸馏}
知识蒸馏是一种通过训练小模型来模仿预训练过的大模型的行为，来达到压缩模型目的的方法。

最早的利用知识转移来压缩模型的方法可以追溯到\citeauthor{buciluǎ2006model}提出的Model Compression\cite{buciluǎ2006model}。
该工作用原大模型生成的伪数据标签训练了另一个更浅的模型，并且重现了大模型的输出结果，达到了压缩模型的效果。
之后\citeauthor{hinton2015distilling}在\parencite{hinton2015distilling}中将基于知识蒸馏的压缩方法发扬光大。
该工作中，更紧凑的学生模型通过同时学习输出标签和教师模型输出的软标签，减轻了训练难度，并压缩了占用的存储空间。
而FitNet\cite{romero2014fitnets}的工作将深而且宽的模型压缩为更深而且狭窄的学生模型。
该工作将训练分成两阶段，第一阶段将教师模型的隐层用于监督学生模型的隐层，第二阶段用教师模型的软标签监督训练学生模型。
FitNet假设学生和教师模型的隐层是类似的，然而这样的假设太强了，因为学生模型和老师模型的不同层很可能有不同的作用。

近年来，随着自然语言处理中的模型越来越庞大，出现了许多利用蒸馏的方法对这些模型进行压缩或在同等资源限制下提高精度的工作。
\parencite{tang2019distilling}在特定任务下把Bert蒸馏到更小的双向LSTM模型，在类似精度的情况下，获得了100倍的压缩比和15倍的加速比。
\parencite{liu2019improving}在每个任务上训练多个MT-DNN，取软标签的平均，
最后用软标签的监督再训练一个MT-DNN，最后在GLUE测试上比BERT优3.2\%。
TinyBert\cite{jiao2019tinybert}提出了一种专门的两段式学习框架，分别在预训练阶段和特定任务的具体学习阶段对Transformer进行蒸馏。
TinyBert以GLUE基准测试下降3\%的代价，将Bert模型存储空间压缩到13\%，将推理速度提高到Bert的9.4倍。