\section{量化}
量化（Quantization）将神经网络的权重从训练时用的32位浮点降低为更低位数的表示，以获得更紧凑的模型和更快的推理速度。
学术界尝试了量化各种不同的可能性。
\cite{gong2014compressing}用标量、向量、残差三种方法尝试了将神经网络的权重用K-Means聚类成长度为$k$的查找表。
\cite{vanhoucke2011improving}展示了用八位量化权重可以在尽量少损失精度的情况下获得较高的加速比。
\cite{gupta2015deep}使用16位定点，并在训练时引入根据小数部分残差定几率的随机取整，
在降低了推理的内存使用量的同时维护了较少的精度损失。

Deep Compression\cite{han2015deep}在\cite{gong2014compressing}使用K-Means的基础上，引入了稀疏化和哈夫曼编码。
该工作第一步根据权重大小将网络稀疏化，再重新训练网络以保持精度。
第二步用聚类算法对网络的权重进行聚类到查找表中，在网络中仅仅存放索引，之后重新训练查找表中的值以保持精度。
最后一步对索引做哈夫曼编码以进一步压缩，并在运行时解码以进一步压缩存储空间。
该工作在在各种之前的量化方法中做到了最好的水准，
在不显著影响精度的前提下，模型压缩比达到了35倍（AlexNet）至 49倍（VGG-19）。

另外，还有许多工作探索了更低精度的量化方法：
二进制神经网络\cite{courbariaux2016binarized}以二进制存放权重，在训练时考虑量化的影响。
三元权重网络\cite{li2016ternary}以$\{-1,0,1\}$作为权重的集合。
XNORNet\cite{rastegari2016xnor}以二进制推理，卷积操作用位运算来进行近似。

而工业界中，更低位数的表示通常是半精度浮点，半精度定点，或者是八位整数。
TensorFlow Lite支持半精度浮点量化，权重八位整数量化，全八位整数量化。
其中半精度浮点量化将权重直接转成半精度浮点，并在模型初始化阶段反量化（Dequantize）；
权重八位整数量化直接将权重转成八位整数，并在推理时将算子输入量化到八位进行运算，并将输出反量化到全精度浮点；
全八位整数量化要求模型的输入输出均为八位整数，并全部使用八位整数推理。
