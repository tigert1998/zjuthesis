\section{自动优化}
\label{automating}
给定多样的调度原语集，我们剩下的问题是为深度学习模型的每一层找到最佳的算子实现。
TVM为每一层模型在不同输入和数据布局下都创建专门的算子。
这种专门化提供了显著的性能优势（手工优化的代码只能针对一小部分输入和数据布局），但是这也带来了自动化方面的挑战。
系统需要选择调度优化（比如修改循环顺序或针对内存层次进行优化），和特定调度的参数，例如分块大小和循环展开因子。
这样组合式的选择为每个硬件后端创建了一个庞大的算子实现空间。
为了应对这一挑战，我们构建了一个具有两个主要组件的自动化调度优化器：
一个提出新配置的调度探索器，和一个预测配置性能的深度学习代价模型。
本节介绍了这些组件以及TVM的自动优化流程（图\ref{fig:auto opt}）。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.9\linewidth]{tvm/auto_opt}
    \caption{\label{fig:auto opt}自动优化框架的整体概览。
    一个调度探索器用基于机器学习的代价模型查看调度空间并选择配置在设备池上进行实验。
    为了提高预测能力，机器学习模型周期性地用在数据库中收集的数据改进自己。}
\end{figure}

\subsection{调度空间规范}
我们构建了一个调度模板规范接口，帮助开发人员在调度空间中声明配置。 
模板规范允许在指定可能的调度时根据需要引入开发人员的特定领域知识。
我们还为每个硬件后端创建了一个通用的主模板，该模板根据使用张量表达语言表达的计算描述自动提取可能的配置。
在高层次上，我们将尽可能地考虑许多配置，并让优化器管理选择负担。
因此，在我们的实验中，优化器必须为现实世界中的深度学习任务搜索数十亿种可能的配置。

\subsection{基于机器学习的代价模型}
从较大配置空间中找到最佳调度的一种方法是通过黑盒优化（即自动调整）。
该方法经常被用于优化高性能计算库。
但是，这样的优化需要大量的实验才能找到良好的配置。

另一种方法是使用预先定义的代价模型，以指导特定硬件后端的搜索，而不是运行所有可能性并评估它们的性能。
理想情况下，一个理想的代价模型应考虑所有影响性能的因素：内存访问模式，数据重用，流水线数据依赖和线程化模式等。
不幸的是，由于现代硬件的日益复杂，这种方法是相当麻烦的。
此外，如果使用这样的方法，则每个新硬件都需要一个新的预定义代价模型。

\begin{table}[htbp]
\small
\begin{tabular}{ccccc}
    \hline
    方法类型 & 数据成本 & 模型偏差 & 是否需要硬件信息 & 是否学习历史数据 \\
    \hline
    黑盒自动优化 & 高 & 无 & 否 & 否 \\
    预先定义的代价模型 & 无 & 高 & 是 & 否 \\
    \textbf{基于机器学习的方法} & \textbf{低} & \textbf{低} & \textbf{否} & \textbf{是} \\
    \hline
\end{tabular}
\caption{\label{tab:methods}自动化方法的比较。模型偏差指建模导致的不精确性。}
\end{table}

相反，我们采用统计的方法来解决成本建模问题。
在这种方法中，调度管理器会提出一些配置，这些配置有可能提高算子的性能。
对于每个调度配置，我们使用基于机器学习的模型，该模型预测降低过的循环程序在给定硬件后端的运行时间。
模型使用在探索期间收集的运行时测量数据进行训练，而不需要用户输入详细的硬件信息。
当我们在优化过程中探索更多配置时，我们会周期性地更新模型，这也会提高预测其他相关工作负载的准确性。
这样以来，随着更多的性能测试，机器学习模型的质量也会逐渐提高。
表\ref{tab:methods}总结了自动化方法之间的主要区别。
基于机器学习的代价模型可以在自动优化和预定义代价模型之间取得平衡，并且可以从相关工作负载的历史性能数据中受益。

\paragraph{机器学习模型选择}
当选择调度探索器应该使用哪种机器学习模型时，我们必须考虑两个关键因素：预测质量和速度。
因为调度探索器经常查询代价模型，所以代价模型的推理和重新拟合会导致不小的开销。
为了使模型实用，这些开销必须小于在实际硬件上测量性能所需的时间，该时间取决于在特定硬件上的工作负载。
这种速度要求将我们的问题与传统的超参数调整问题区分开。
在传统的超参数调整问题中，执行测量的成本相对模型推理开销来说非常高，因而可以使用计算复杂度更高的模型。
除了选择模型之外，我们还需要选择一个目标函数来训练模型，例如运行时间的预测误差。
但是，由于探索器仅根据预测的性能相对顺序（A的运行速度比B快）选择了最高的候选者，因此不必直接预测绝对执行时间。
相反，我们使用相对性能作为目标函数来指导调度选择。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.9\linewidth]{tvm/auto_methods}
    \caption{\label{fig:auto methods}在ResNet-18中的一个二维卷积算子上比较不同的自动化方法，测试在Titan X上进行。
    该基于机器学习的模型开始时没有训练数据，之后用收集的数据提高它自己。
    Y坐标轴是和cuDNN相比较的加速比。我们观察到在其他工作负载上有类似的趋势。}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=.9\linewidth]{tvm/cost_model_workflow}
    \caption{\label{fig:cost model workflow}机器学习代价模型的工作流示意。
    XGBoost基于循环程序的特性预测代价。TreeRNN直接总结抽象语法树。}
\end{figure}

我们在机器学习优化器中实现了几种类型的模型。
我们使用了梯度提升树（基于XGBoost），该模型基于从循环程序中提取的特征进行预测；
这些特征包括每个循环层级的每个内存缓冲区的内存访问计数和重用率，
以及循环标注的独热码，例如``向量化''，``循环展开''和``并行''。
我们评估了一个神经网络模型，在评估中，我们使用TreeRNN来总结循环程序的抽象语法树，而无需进行特征工程。
图\ref{fig:cost model workflow}总结了代价模型的运作流程。
我们发现，提升树和TreeRNN具有相似的预测质量。
但是，前者的预测的速度快一倍，并且训练时间更少。
所以我们在实验中选择了梯度提升树作为默认的代价模型。
尽管如此，我们认为这两种方法都是有价值的，并且值得未来在这个问题上进行进一步的研究。
平均而言，提升树模型可以在0.67毫秒内完成预测，比运行实际测量快数千倍。
图\ref{fig:auto methods}将基于机器学习的优化器与黑盒自动调整方法进行了比较。
前者比后者能更快地找到更好的配置。

\subsection{调度探索}
一旦选择了代价模型，我们就可以使用它来选择性能优秀的配置，以迭代地对其进行的实际测量。
在每次迭代中，探索器都会使用机器学习模型的预测来选择一组候选对象进行测量。
然后将收集的数据用作训练数据以更新模型。
如果没有初始训练数据，则可以随机选择候选对象。

最简单的探索算法会枚举每个配置并预测性能，然后选择排名前$k$位的候选对象。
但是，这种策略在搜索空间较大时会变得难以使用。
相反，我们使用模拟退火算法来解决这个问题。
探索器从随机配置开始，然后在每次迭代中随机漫游到其他配置。
如果真实运行代价和代价模型的预测一致并且是降低的，则此迁移成功。
但如果下一步目标配置的实际运行代价更高，则可能会拒绝这一步迭代。
随机游走更倾向于在真实代价更低的配置上收敛，这和代价模型的预测一致。
探索器在代价模型的更新中保持状态连续；
在模型更新之后，我们将从上一次的配置继续搜索。

\subsection{分布式设备池和远程过程调用}
分布式设备池可以自动伸缩在硬件上性能测试的吞吐量，并且允许在多个优化任务之间进行细粒度的资源共享。
TVM实现了一个基于远程过程调用的定制的分布式设备池，该池使客户端能够在特定类型的设备上运行程序。 
我们可以使用此接口在主机编译器上编译程序，请求远程设备，运行远程函数，并在主机上以相同的脚本访问结果。 
TVM的远程过程调用支持动态上传，运行交叉编译的模块和函数。 
结果是，相同的基础设施可以执行单个工作负载优化和端到端计算图推理。 
我们的方法可以自动跨多个设备进行编译，运行和测量。
这种基础架构对嵌入式设备特别重要，从前这些设备需要繁琐的手工工作才能进行交叉编译，代码部署和测量。