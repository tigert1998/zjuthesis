\section{生成张量操作}
TVM给每个硬件后端生成许多高效的实现并选择其中一个优化的实现，来为每个算子生成高效的代码。
此过程建立在Halide的解耦描述与计算规则（或者调度优化）思想的基础上，
并将其扩展为支持新的优化（嵌套并行性，张量化和延迟隐藏）和各种硬件后端。
现在，我们重点介绍TVM特性。

\subsection{张量表达式和调度空间}
我们引入了一个张量表达式语言来支持自动代码生成。与不透明的高级计算图表示不同，
在张量表达式中，每个运算都是用索引公式语言来表达的。以下代码显示了一个张量表达式示例，
用于计算转置矩阵乘法：

\begin{lstlisting}[language={Python}]
m, n, h = t.var('m'), t.var('n'), t.var('h')
A = t.placeholder((m, h), name='A')
B = t.placeholder((n, h), name='B')
k = t.reduce_axis((0, h), name='k')
C = t.compute((m, n), lambda y, x:
    t.sum(A[k, y] * B[k, x], axis=k)) 
\end{lstlisting}

每个计算操作都指定输出张量的形状，描述了如何计算其中的每个元素。
我们的张量表达式语言支持常见的算术和数学运算，并涵盖了常见的深度学习算子。
该语言没有指定循环结构和许多其他执行细节，它为添加各种后端的硬件优化提供了灵活性。
我们采用了Halide中解耦计算/调度的思想，使用调度来表示从张量表达式到低级代码的过程。
对同一个函数，存在许多合法的调度。

我们通过逐步应用保持等效性的基本转换（调度原语）来构建调度。
图 显示了在专用加速器上调度矩阵乘法的示例。
TVM内部在应用基本转换时会使用数据结构来跟踪循环结构和其他信息。
然后，这些信息可以帮助为给定的最终调度生成底层代码。

我们的张量表达式继承自Halide，Darkroom和TACO。
它的增强功能主要包括对下面讨论的对新调度优化的支持。
为了在许多后端上实现高性能，我们必须支持足够的调度原语，以涵盖不同硬件后端上的各种优化。
图 总结了算子的代码生成过程和TVM支持的调度原语。
我们重用Halide中有用的原语和低层次循环AST，并引入新的原语来优化GPU和加速器性能。
新的原语是实现最佳GPU性能所必需的，并且对于硬件加速器至关重要。
CPU，GPU，类TPU的加速器是深度学习的三种重要的硬件类型。
本节介绍了针对CPU，GPU和类TPU的加速器的新的优化原语，而第\ref{automating}节
则说明了如何自动得出高效的调度。

\subsection{合作的嵌套循环}
在深度学习工作负载中，并行化是提高计算密集型内核效率的关键。
现代GPU提供大规模并行性，这要求我们把并行的模式嵌入到调度转换原语中。
大多数现有的解决方案都采用嵌套并行性的模型，即Fork-Join的一种形式。
该模型需要并行调度原语来并行化一个数据并行任务。
每个任务都可以进一步地递归划分为子任务，以利用目标体系结构的多级线程层次结构（例如GPU中的线程组）。
我们称此模型为无共享的嵌套并行性，因为在同一并行计算阶段，一个工作线程无法查看其他线程的数据。

无共享方法的替代方法是协作获取数据。
具体来说，线程组可以协作地获取它们全部需要的数据，并将其放置在共享的内存空间中。
此优化可以利用GPU内存的层次结构，通过共享内存区域来做到跨线程重用数据。
TVM使用调度原语来支持GPU优化，以实现最佳性能。
以下GPU代码示例优化了矩阵乘法。

\begin{lstlisting}[language={Python}]
for thread_group (by, bx) in cross(64, 64):
    for thread_item (ty, tx) in cross(2, 2):
        local CL[8][8] = 0
        shared AS[2][8], BS[2][8]
        for k in range(1024):
            for i in range(4):
                AS[ty][i*4+tx] = A[k][by*64+ty*8+i*4+tx]
            for each i in 0..4:
                BS[ty][i*4+tx] = B[k][bx*64+ty*8+i*4+tx]
            memory_barrier_among_threads()
            for yi in range(8):
                for xi in range(8):
                    CL[yi][xi] += AS[yi] * BS[xi]
            for yi in range(8):
                for xi in range(8):
                    C[yo*8+yi][xo*8+xi] = CL[yi][xi] 
\end{lstlisting}

图 演示了此优化的作用。
我们将内存作用域的概念引入调度空间，以便一个计算阶段（代码中的AS和BS）可以被标记成共享的。
如果没有显式内存作用域，那么自动内存作用域推断会将计算阶段标记为线程本地的。
共享任务必须计算组中所有工作线程的依赖关系。
此外，内存同步屏障必须被正确地插入，以确保共享的加载数据对所有消费者可见。
最后，除了对GPU有用之外，内存作用域还允许我们标记特殊的内存缓冲区并针对专用深度学习加速器创建特殊的
生成低级代码规则。

\subsection{张量化}

\subsection{显式内存延时隐藏}