\section{生成张量操作}
TVM给每个硬件后端生成许多高效的实现并选择其中一个优化的实现，来为每个算子生成高效的代码。
此过程建立在Halide的解耦描述与计算规则（或者调度优化）思想的基础上，
并将其扩展为支持新的优化（嵌套并行性，张量化和延迟隐藏）和各种硬件后端。
现在，我们重点介绍TVM特性。

\subsection{张量表达式和调度空间}
我们引入了一个张量表达式语言来支持自动代码生成。与不透明的高级计算图表示不同，
在张量表达式中，每个运算都是用索引公式语言来表达的。以下代码显示了一个张量表达式示例，
用于计算转置矩阵乘法：

\begin{lstlisting}[language={Python}]
m, n, h = t.var('m'), t.var('n'), t.var('h')
A = t.placeholder((m, h), name='A')
B = t.placeholder((n, h), name='B')
k = t.reduce_axis((0, h), name='k')
C = t.compute((m, n), lambda y, x:
    t.sum(A[k, y] * B[k, x], axis=k)) 
\end{lstlisting}

每个计算操作都指定输出张量的形状，描述了如何计算其中的每个元素。
我们的张量表达式语言支持常见的算术和数学运算，并涵盖了常见的深度学习算子。
该语言没有指定循环结构和许多其他执行细节，它为添加各种后端的硬件优化提供了灵活性。
我们采用了Halide中解耦计算/调度的思想，使用调度来表示从张量表达式到底层代码的过程。
对同一个函数，存在许多合法的调度。

我们通过逐步应用保持等效性的基本转换（调度原语）来构建调度。
图 显示了在专用加速器上调度矩阵乘法的示例。
TVM内部在应用基本转换时会使用数据结构来跟踪循环结构和其他信息。
然后，这些信息可以帮助为给定的最终调度生成底层代码。

我们的张量表达式继承自Halide，Darkroom和TACO。
它的增强功能主要包括对下面讨论的对新调度优化的支持。
为了在许多后端上实现高性能，我们必须支持足够的调度原语，以涵盖不同硬件后端上的各种优化。
图 总结了算子的代码生成过程和TVM支持的调度原语。
我们重用Halide中有用的原语和低层次循环AST，并引入新的原语来优化GPU和加速器性能。
新的原语是实现最佳GPU性能所必需的，并且对于硬件加速器至关重要。
CPU，GPU，类TPU的加速器是深度学习的三种重要的硬件类型。
本节介绍了针对CPU，GPU和类TPU的加速器的新的优化原语，而第\ref{automating}节
则说明了如何自动得出高效的调度。

\subsection{合作的嵌套循环}
在深度学习工作负载中，并行化是提高计算密集型内核效率的关键。
现代GPU提供大规模并行性，这要求我们把并行的模式嵌入到调度转换原语中。
大多数现有的解决方案都采用嵌套并行性的模型，即Fork-Join的一种形式。
该模型需要并行调度原语来并行化一个数据并行任务。
每个任务都可以进一步地递归划分为子任务，以利用目标体系结构的多级线程层次结构（例如GPU中的线程组）。
我们称此模型为无共享的嵌套并行性，因为在同一并行计算阶段，一个工作线程无法查看其他线程的数据。

无共享方法的替代方法是协作获取数据。
具体来说，线程组可以协作地获取它们全部需要的数据，并将其放置在共享的内存空间中。
此优化可以利用GPU内存的层次结构，通过共享内存区域来做到跨线程重用数据。
TVM使用调度原语来支持GPU优化，以实现最佳性能。
以下GPU代码示例优化了矩阵乘法。

\begin{lstlisting}[language={Python}]
for thread_group (by, bx) in cross(64, 64):
    for thread_item (ty, tx) in cross(2, 2):
        local CL[8][8] = 0
        shared AS[2][8], BS[2][8]
        for k in range(1024):
            for i in range(4):
                AS[ty][i*4+tx] = A[k][by*64+ty*8+i*4+tx]
            for each i in 0..4:
                BS[ty][i*4+tx] = B[k][bx*64+ty*8+i*4+tx]
            memory_barrier_among_threads()
            for yi in range(8):
                for xi in range(8):
                    CL[yi][xi] += AS[yi] * BS[xi]
            for yi in range(8):
                for xi in range(8):
                    C[yo*8+yi][xo*8+xi] = CL[yi][xi] 
\end{lstlisting}

图 演示了此优化的作用。
我们将内存作用域的概念引入调度空间，以便一个计算阶段（代码中的AS和BS）可以被标记成共享的。
如果没有显式内存作用域，那么自动内存作用域推断会将计算阶段标记为线程本地的。
共享任务必须计算组中所有工作线程的依赖关系。
此外，内存同步屏障必须被正确地插入，以确保共享的加载数据对所有消费者可见。
最后，除了对GPU有用之外，内存作用域还允许我们标记特殊的内存缓冲区并针对专用深度学习加速器创建特殊的
生成底层代码规则。

\subsection{张量化}
深度学习工作负载具有很高的算术强度，它们通常可以被分解为张量运算，例如矩阵乘法或一维卷积。
这样的分解导致张量计算原语与日俱增。这些新的原语为基于调度的编译带来了机遇和挑战。
尽管使用它们可以提高性能，但编译框架必须无缝集成它们。
我们称之为张量化：它类似于SIMD体系结构的矢量化，但有和它显著差异。
指令输入是多维的，长度固定或可变，并且每个输入都有不同的数据布局。
更重要的是，我们无法支持一组固定的原语，因为新的加速器正在出现，它们各自有各自的张量指令。
因此，我们需要一个可扩展的解决方案。

我们通过使用张量内参声明机制将目标硬件内参与调度分开来使得张量可扩展。
我们使用相同的张量表达式语言来声明每个新硬件的内参以及它的代码降低（Lowering）规则。
以下代码显示了如何声明一个$8\times 8$张量硬件内参。

\begin{lstlisting}[language={Python}]
w, x = t.placeholder((8, 8)), t.placeholder((8, 8))
k = t.reduce_axis((0, 8))
y = t.compute((8, 8), lambda i, j:
    t.sum(w[i, k] * x[j, k], axis=k))
def gemm_intrin_lower(inputs, outputs):
    ww_ptr = inputs[0].access_ptr("r")
    xx_ptr = inputs[1].access_ptr("r")
    zz_ptr = outputs[0].access_ptr("w")
    compute = t.hardware_intrin("gemm8x8", ww_ptr, xx_ptr, zz_ptr)
    reset = t.hardware_intrin("fill_zero", zz_ptr)
    update = t.hardware_intrin("fuse_gemm8x8_add", ww_ptr, xx_ptr, zz_ptr)
    return compute, reset, update
gemm8x8 = t.decl_tensor_intrin(y.op, gemm_intrin_lower) 
\end{lstlisting}

此外，我们引入了张量化调度原语来替换一组包含内参的计算单元。
编译器将计算模式与硬件声明进行匹配，根据相应的硬件内参降低代码层次。

张量化将调度与特定的硬件原语解耦，从而轻松扩展TVM以支持新的硬件架构。
张量调度生成的代码与高性能计算中的常用操作保持一致：将复杂的操作分解为一系列的微内核调用。
我们还可以使用张量化原语来利用手工设计的微内核，这在某些平台上可能是有效的。
例如，我们利用位串行矩阵向量乘法微内核，为移动端CPU实现了超低精度算子，这些算子可对一位或两位宽的数据类型进行操作。
该微内核逐渐将结果累积为越来越大的数据类型，以最大程度地减少内存占用。
将微内核表示为TVM的张量内参，与未张量化的版本相比，达到了1.5的加速比。

\subsection{显式内存延时隐藏}
延迟隐藏指重叠内存操作与计算过程，以最大程度地利用内存和计算资源。
根据目标硬件后端，它需要不同的策略。
在CPU上，内存延迟隐藏是通过同时多线程或硬件预取隐式实现的。
而GPU上的内存延迟隐藏的实现依赖于线程束的快速上下文切换。
相比之下，专用的深度学习加速器（例如TPU）通常倾向于使用解耦访问控制（DAE）架构的精益控制，
并且将细粒度同步的问题交给软件处理。

图 显示了减少延时的DAE硬件的流水线。
与单片硬件设计相比，流水线可以隐藏大多数内存访问开销，几乎可以充分利用计算资源。
为了实现更高的利用率，指令流必须用细粒度的同步操作扩展。
没有它们，依赖性就不能被添加，错误的执行就会发生。
因此，DAE硬件流水线需要在流水线阶段之间进行细粒度的依赖入队/出队操作，以确保正确执行，如图 的指令流所示。

对需要显式底层同步的DAE加速器进行编程是困难的。
为了减轻编程负担，我们引入了虚拟线程调度原语，它使程序员可以指定高层次数据并行程序，
就像它们在支持多线程的硬件后端一样。
然后，TVM自动将程序降低为具有底层显式同步的单个指令流，如图 所示。
该算法从高层次的多线程程序调度开始，插入必要的底层同步操作以确保每个线程被正确地执行。
接下来，它将所有虚拟线程的操作交织到单个指令流中。
最后，硬件恢复由指令流中的底层同步操作决定的可用流水线并行性。

\paragraph{延迟隐藏的硬件评估}
现在，我们演示了在FPGA加速器上隐藏延迟的有效性。
我们在加速器上运行ResNet的每一层，并使用TVM生成两个调度：一个隐藏了延迟，另一个没有。
带有延迟隐藏的调度将程序用虚拟线程并行化，以激发流水线并行性，从而隐藏内存访问延迟。
结果如图 所示。屋顶线性能图可以可视化给定系统在不同基准测试中使用计算和内存资源的程度。
总体而言，延迟隐藏提高了所有ResNet算子上的性能。
峰值计算利用率从无延迟隐藏的70\%增加到有延迟隐藏的88\%。
